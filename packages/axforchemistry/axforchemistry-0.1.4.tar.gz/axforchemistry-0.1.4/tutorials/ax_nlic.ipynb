{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d071cff",
   "metadata": {},
   "source": [
    "## Using non-linear inequality constraints in Ax\n",
    "This notebook comes with the following caveats:\n",
    "1. The search space has to be [0, 1]^d\n",
    "2. We need to pass in explicit `batch_initial_conditions` that satisfy the non-linear inequality constraints as starting points for optimizing the acquisition function.\n",
    "3. BATCH_SIZE must be equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdfa40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.transforms import Standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "from ax import (\n",
    "    Data,\n",
    "    Experiment,\n",
    "    ParameterType,\n",
    "    RangeParameter,\n",
    "    SearchSpace,\n",
    "    SumConstraint,\n",
    ")\n",
    "\n",
    "from ax.storage.json_store.save import save_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bf1df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.core.objective import Objective\n",
    "from ax.core.optimization_config import OptimizationConfig\n",
    "\n",
    "# from ax.metrics.hartmann6 import Hartmann6Metric\n",
    "from axforchemistry.utils.metrics import (\n",
    "    CompositionalHartmann6Metric,\n",
    "    NoisyCompositionalHartmann6Metric,\n",
    "    extraordinary_probability,\n",
    ")\n",
    "\n",
    "# from ax.utils.measurement.synthetic_functions import Hartmann6\n",
    "from ax.modelbridge.registry import Models\n",
    "from ax.runners.synthetic import SyntheticRunner\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "from axforchemistry.utils.sobol import nchoosek_sobol\n",
    "\n",
    "dummy = False\n",
    "\n",
    "result_dir = \"results\"\n",
    "Path(result_dir).mkdir(exist_ok=True)\n",
    "\n",
    "noise_sd = 0.1\n",
    "synth_dither = 0.1\n",
    "sem = None\n",
    "\n",
    "d = 5  # HARD-CODED PARAMETER, i.e. 5 + 1 = 6 for Hartmann6Metric\n",
    "param_names = [f\"x{i}\" for i in range(d + 1)]\n",
    "subparam_names = param_names[:-1]  # sub-parameter names (i.e. all but last component)\n",
    "params = [\n",
    "    RangeParameter(\n",
    "        name=parameter_name,\n",
    "        parameter_type=ParameterType.FLOAT,\n",
    "        lower=0.0,\n",
    "        upper=1.0,\n",
    "    )\n",
    "    for parameter_name in subparam_names\n",
    "]\n",
    "search_space = SearchSpace(\n",
    "    parameters=params,\n",
    "    parameter_constraints=[\n",
    "        SumConstraint(parameters=params, is_upper_bound=True, bound=1.0)\n",
    "    ],\n",
    ")\n",
    "\n",
    "metric = NoisyCompositionalHartmann6Metric(\n",
    "    name=\"objective\",\n",
    "    param_names=subparam_names,\n",
    "    noise_sd=noise_sd,\n",
    "    synth_dither=synth_dither,\n",
    "    sem=sem,\n",
    "    n=10,\n",
    "    seed=10,\n",
    ")\n",
    "optimization_config = OptimizationConfig(\n",
    "    objective=Objective(\n",
    "        metric=metric,\n",
    "        minimize=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0dec9b",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Let's see how we do via a brute force search"
   },
   "outputs": [],
   "source": [
    "if dummy:\n",
    "    comb_m = 10\n",
    "else:\n",
    "    comb_m = 18\n",
    "candidates = nchoosek_sobol(\n",
    "    param_names, n_slots=3, comb_m=comb_m, fixed_compositions=False\n",
    ")\n",
    "print(f\"{len(candidates)} SOBOL candidates generated\")\n",
    "# compute the dither all at once, and add it to hartmann6 to get \"true\" fn\n",
    "dither = metric.interp(candidates)\n",
    "noise_free = metric.f_without_dither\n",
    "ys = [noise_free(x) for x in candidates.values[:, :5]]\n",
    "ys = np.array(ys) + dither\n",
    "idx = np.argmin(ys)\n",
    "print(f\"minimum estimated via SOBOL search with true values: {ys[idx]:.4f}\")\n",
    "x_opt = candidates.iloc[idx]\n",
    "\n",
    "# probability of finding a candidate within some percent of the estimated optimum\n",
    "ys_noise = ys + noise_sd * np.random.randn(len(ys))\n",
    "# for seemingly extraordinary candidates, do repeats to verify (i.e. with true values)\n",
    "# mn = min(ys)\n",
    "# mx = max(ys)\n",
    "mn = -1.484  # as estimated by SAASBO\n",
    "print(f\"minimum estimated previously by SAASBO: {mn:.3f}\")\n",
    "mx = 0.0\n",
    "thresh = 0.10  # i.e. within 10% of optimum\n",
    "\n",
    "extraordinary_probability(ys, ys_noise, mx=mx, mn=mn, thresh=thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b5fd71",
   "metadata": {},
   "source": [
    "We want to optimize $f_{\\text{hartmann6}}(x)$ subject to an additional constraint $|| x ||_0 <= 3$.\n",
    "\n",
    "This constraint isn't differentiable, but it can be approximated by a differentiable relaxation using a sum of narrow Gaussian basis functions.\n",
    "Given a univariate Gaussian basis function $g_{\\ell}(x)$ centered at zero with $\\ell > 0$ small,\n",
    "we can approximate the constraint by: $|| x ||_0 \\approx 6 - \\sum_{i=1}^6 g_{\\ell}(x_i) \\leq 3$, which reduces to $\\sum_{i=1}^6 g_{\\ell}(x_i) \\geq 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e20438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def narrow_gaussian(x, ell):\n",
    "    return torch.exp(-0.5 * (x / ell) ** 2)\n",
    "\n",
    "\n",
    "def ineq_constraint(x, ell=1e-3):\n",
    "    # Approximation of || x ||_0 <= 3. The constraint is >= 0 to conform with SLSQP\n",
    "    return narrow_gaussian(x, ell).sum(dim=-1) - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6d316",
   "metadata": {},
   "source": [
    "## BO-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90b4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_initial_conditions(n, X, Y, raw_samples):\n",
    "    \"\"\"Generate starting points for the acquisition function optimization.\"\"\"\n",
    "    # 1. Draw `raw_samples` Sobol points and randomly set three parameters to zero to satisfy the constraint\n",
    "    X_cand = SobolEngine(dimension=d, scramble=True).draw(raw_samples)\n",
    "    X_cand = normalize(X_cand).to(torch.double)\n",
    "    inds = torch.argsort(torch.rand(raw_samples, d), dim=-1)[:, :3]\n",
    "    X_cand[torch.arange(X_cand.shape[0]).unsqueeze(-1), inds] = 0\n",
    "\n",
    "    # 2. Fit a GP to the observed data, the right thing to do is to use the Ax model here\n",
    "    gp = SingleTaskGP(X, Y, outcome_transform=Standardize(m=1))\n",
    "    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)\n",
    "    fit_gpytorch_model(mll)\n",
    "\n",
    "    # 3. Use EI to select the best points. Ideally, we should use the Ax acquisition function here as well\n",
    "    EI = ExpectedImprovement(model=gp, best_f=Y.min(), maximize=False)\n",
    "    X_cand = X_cand.unsqueeze(1)\n",
    "    acq_vals = EI(X_cand)\n",
    "    return X_cand[acq_vals.topk(n).indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "if dummy:\n",
    "    N_INIT = 5\n",
    "    N_BATCHES = 2\n",
    "else:\n",
    "    N_INIT = 10\n",
    "    N_BATCHES = 90\n",
    "print(f\"Doing {N_INIT + N_BATCHES * BATCH_SIZE} evaluations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122ba711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment\n",
    "experiment = Experiment(\n",
    "    name=\"saasbo_experiment\",\n",
    "    search_space=search_space,\n",
    "    optimization_config=optimization_config,\n",
    "    runner=SyntheticRunner(),\n",
    ")\n",
    "\n",
    "# Initial Sobol points (set three random parameters to zero)\n",
    "sobol = Models.SOBOL(search_space=experiment.search_space)\n",
    "for _ in range(N_INIT):\n",
    "    trial = sobol.gen(1)\n",
    "    keys = copy(subparam_names)\n",
    "    random.shuffle(keys)\n",
    "    for k in keys[:3]:\n",
    "        trial.arms[0]._parameters[k] = 0.0\n",
    "    experiment.new_trial(trial).run()\n",
    "\n",
    "# Run SAASBO\n",
    "data = experiment.fetch_data()\n",
    "for i in range(N_BATCHES):\n",
    "    model = Models.FULLYBAYESIAN(\n",
    "        experiment=experiment,\n",
    "        data=data,\n",
    "        num_samples=256,  # Increasing this may result in better model fits\n",
    "        warmup_steps=512,  # Increasing this may result in better model fits\n",
    "        gp_kernel=\"matern\",  # \"rbf\" is the default in the paper, but we also support \"matern\"\n",
    "        torch_dtype=torch.double,\n",
    "        verbose=False,  # Set to True to print stats from MCMC\n",
    "        disable_progbar=True,  # Set to False to print a progress bar from MCMC\n",
    "    )\n",
    "    batch_initial_conditions = get_batch_initial_conditions(\n",
    "        n=20, X=model.model.Xs[0], Y=model.model.Ys[0], raw_samples=1024\n",
    "    )\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # Filter SLSQP warnings\n",
    "        generator_run = model.gen(\n",
    "            BATCH_SIZE,\n",
    "            model_gen_options={\n",
    "                \"optimizer_kwargs\": {\n",
    "                    \"linear_constraints\": [\n",
    "                        (torch.arange(d), torch.ones(d), 1)\n",
    "                    ],  # sum(x[:-1]) <= 1\n",
    "                    \"nonlinear_inequality_constraints\": [ineq_constraint],\n",
    "                    \"batch_initial_conditions\": batch_initial_conditions,\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "\n",
    "    trial = experiment.new_batch_trial(generator_run=generator_run)\n",
    "    for arm in trial.arms:\n",
    "        arm._parameters = {k: 0.0 if v < 1e-3 else v for k, v in arm.parameters.items()}\n",
    "        assert sum([v > 1e-3 for v in arm.parameters.values()]) <= 3\n",
    "    trial.run()\n",
    "    data = Data.from_multiple_data([data, trial.fetch_data()])\n",
    "\n",
    "    fetched_data = trial.fetch_data()\n",
    "    new_value = fetched_data.df[\"mean\"].min()\n",
    "    # best_value = fetched_data.true_df[\"mean\"].min()\n",
    "    best_value = data.df[\"mean\"].min()\n",
    "\n",
    "    arm_parameters = [arm.parameters for arm in list(experiment.arms_by_name.values())]\n",
    "    arm_params = pd.DataFrame(arm_parameters).values\n",
    "    y_true = np.array([metric.f(v) for v in arm_params])\n",
    "    best_true_val = min(y_true)\n",
    "    print(\n",
    "        f\"Iteration: {i}, Best in iteration {new_value:.3f}, \",\n",
    "        f\"Best so far: {best_value:.3f}, \",\n",
    "        f\"Best true so far: {best_true_val:.3f}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839bc2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "df = pd.DataFrame(arm_parameters)\n",
    "df[\"x5\"] = np.round(1 - df.values.sum(axis=1), decimals=6)\n",
    "y_pred = data.df[\"mean\"]\n",
    "df[\"y_pred\"] = y_pred\n",
    "df[\"y_true\"] = y_true\n",
    "print(df)\n",
    "\n",
    "# y_pred = df[]\n",
    "extraordinary_probability(y_true, y_pred, mx=mx, mn=mn)\n",
    "\n",
    "experiment_dir = result_dir\n",
    "if dummy:\n",
    "    experiment_dir = join(\"dummy\", experiment_dir)\n",
    "experiment_dir = join(\n",
    "    experiment_dir,\n",
    "    \"experiments\",\n",
    "    f\"{experiment.name}\",\n",
    "    f\"N_INIT_{N_INIT}_BATCH_SIZE_{BATCH_SIZE}_N_BATCHES_{N_BATCHES}\",\n",
    ")\n",
    "Path(experiment_dir).mkdir(exist_ok=True, parents=True)\n",
    "experiment_fpath = join(experiment_dir, \"experiment.json\")\n",
    "save_experiment(experiment, experiment_fpath)\n",
    "\n",
    "df.to_csv(join(experiment_dir, \"results.csv\"))\n",
    "\n",
    "# TODO: Implement distance cutoff within which to consider extraordinary compounds equivalent\n",
    "# TODO: print best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87310b9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df6e0d",
   "metadata": {
    "title": "Code Graveyard"
   },
   "outputs": [],
   "source": [
    "# \"equality_constraints\": [\n",
    "#     (torch.arange(6), torch.ones(6), 1)\n",
    "# ],  # sum(x) == 1"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
