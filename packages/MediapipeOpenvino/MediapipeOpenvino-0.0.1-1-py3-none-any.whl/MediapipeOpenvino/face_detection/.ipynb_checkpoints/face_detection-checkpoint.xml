<?xml version="1.0" ?>
<net name="torch-jit-export" version="11">
	<layers>
		<layer id="0" name="input" type="Parameter" version="opset1">
			<data shape="?,3,128,128" element_type="f32"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="input"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="input">
					<dim>-1</dim>
					<dim>3</dim>
					<dim>128</dim>
					<dim>128</dim>
				</port>
			</output>
		</layer>
		<layer id="1" name="Split_146.0" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="0" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="2" name="Split_146.1" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="32" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="3" name="97" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="64" size="4"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="97"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="97"/>
			</output>
		</layer>
		<layer id="4" name="98" type="Pad" version="opset1">
			<data pad_mode="constant"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="98, Split_146"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>3</dim>
					<dim>128</dim>
					<dim>128</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
				<port id="2" precision="I64">
					<dim>4</dim>
				</port>
				<port id="3" precision="FP32"/>
			</input>
			<output>
				<port id="4" precision="FP32" names="98">
					<dim>-1</dim>
					<dim>3</dim>
					<dim>131</dim>
					<dim>131</dim>
				</port>
			</output>
		</layer>
		<layer id="5" name="backbone1.0.weight" type="Const" version="opset1">
			<data element_type="f32" shape="24, 3, 5, 5" offset="68" size="7200"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone1.0.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone1.0.weight">
					<dim>24</dim>
					<dim>3</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</output>
		</layer>
		<layer id="6" name="Convolution_218" type="Convolution" version="opset1">
			<data strides="2, 2" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_218"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>3</dim>
					<dim>131</dim>
					<dim>131</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>24</dim>
					<dim>3</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="7" name="Reshape_238" type="Const" version="opset1">
			<data element_type="f32" shape="1, 24, 1, 1" offset="7268" size="96"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="8" name="102" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="102, Concat_237, Reshape_238"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="102">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="9" name="103" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="103"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="103">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="10" name="Reshape_274" type="Const" version="opset1">
			<data element_type="f32" shape="24, 1, 1, 3, 3" offset="7364" size="864"/>
			<output>
				<port id="0" precision="FP32">
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="11" name="GroupConvolution_340" type="GroupConvolution" version="opset1">
			<data strides="1, 1" pads_begin="1, 1" pads_end="1, 1" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="104, Concat_359, GroupConvolution_340, Reshape_274, Reshape_360"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="104">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="12" name="backbone1.2.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="24, 24, 1, 1" offset="8228" size="2304"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone1.2.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone1.2.convs.1.weight">
					<dim>24</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="13" name="Convolution_388" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_388"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>24</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="14" name="Reshape_408" type="Const" version="opset1">
			<data element_type="f32" shape="1, 24, 1, 1" offset="10532" size="96"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="15" name="105" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="105, Concat_407, Reshape_408"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="105">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="16" name="106" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="106"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="106">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="17" name="107" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="107"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="107">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="18" name="Reshape_546" type="Const" version="opset1">
			<data element_type="f32" shape="24, 1, 1, 3, 3" offset="10628" size="864"/>
			<output>
				<port id="0" precision="FP32">
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="19" name="GroupConvolution_612" type="GroupConvolution" version="opset1">
			<data strides="1, 1" pads_begin="1, 1" pads_end="1, 1" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="132, Concat_631, GroupConvolution_612, Reshape_546, Reshape_632"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="132">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="20" name="backbone1.3.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="28, 24, 1, 1" offset="11492" size="2688"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone1.3.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone1.3.convs.1.weight">
					<dim>28</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="21" name="Convolution_660" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_660"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>28</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="22" name="Reshape_680" type="Const" version="opset1">
			<data element_type="f32" shape="1, 28, 1, 1" offset="14180" size="112"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>28</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="23" name="133" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="133, Concat_679, Reshape_680"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>28</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="133">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="24" name="Split_471.0" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14292" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="25" name="Split_471.1" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14324" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="26" name="130" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="64" size="4"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="130"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="130"/>
			</output>
		</layer>
		<layer id="27" name="131" type="Pad" version="opset1">
			<data pad_mode="constant"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="131, Split_471"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>24</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
				<port id="2" precision="I64">
					<dim>4</dim>
				</port>
				<port id="3" precision="FP32"/>
			</input>
			<output>
				<port id="4" precision="FP32" names="131">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="28" name="134" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="134"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="134">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="29" name="135" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="135"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="135">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</output>
		</layer>
		<layer id="30" name="Reshape_920" type="Const" version="opset1">
			<data element_type="f32" shape="28, 1, 1, 3, 3" offset="14356" size="1008"/>
			<output>
				<port id="0" precision="FP32">
					<dim>28</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="31" name="GroupConvolution_986" type="GroupConvolution" version="opset1">
			<data strides="2, 2" pads_begin="0, 0" pads_end="2, 2" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="159, 185, Concat_1005, GroupConvolution_986, Reshape_1006, Reshape_920, Split_743"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>28</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="185">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="32" name="backbone1.4.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="32, 28, 1, 1" offset="15364" size="3584"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone1.4.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone1.4.convs.1.weight">
					<dim>32</dim>
					<dim>28</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="33" name="Convolution_1034" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_1034"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>32</dim>
					<dim>28</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="34" name="Reshape_1054" type="Const" version="opset1">
			<data element_type="f32" shape="1, 32, 1, 1" offset="18948" size="128"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>32</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="35" name="186" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="186, Concat_1053, Reshape_1054"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>32</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="186">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="36" name="160" type="MaxPool" version="opset8">
			<data strides="2, 2" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" kernel="2, 2" rounding_type="floor" auto_pad="explicit" index_element_type="i64" axis="0"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="160"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>64</dim>
					<dim>64</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="160">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="2" precision="I64">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="37" name="Split_845.0" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14292" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="38" name="Split_845.1" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14324" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="39" name="183" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="64" size="4"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="183"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="183"/>
			</output>
		</layer>
		<layer id="40" name="184" type="Pad" version="opset1">
			<data pad_mode="constant"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="184, Split_845"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>28</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
				<port id="2" precision="I64">
					<dim>4</dim>
				</port>
				<port id="3" precision="FP32"/>
			</input>
			<output>
				<port id="4" precision="FP32" names="184">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="41" name="187" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="187"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="187">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="42" name="188" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="188"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="188">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="43" name="Reshape_1192" type="Const" version="opset1">
			<data element_type="f32" shape="32, 1, 1, 3, 3" offset="19076" size="1152"/>
			<output>
				<port id="0" precision="FP32">
					<dim>32</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="44" name="GroupConvolution_1258" type="GroupConvolution" version="opset1">
			<data strides="1, 1" pads_begin="1, 1" pads_end="1, 1" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="213, Concat_1277, GroupConvolution_1258, Reshape_1192, Reshape_1278"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>32</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="213">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="45" name="backbone1.5.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="36, 32, 1, 1" offset="20228" size="4608"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone1.5.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone1.5.convs.1.weight">
					<dim>36</dim>
					<dim>32</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="46" name="Convolution_1306" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_1306"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>36</dim>
					<dim>32</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>36</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="47" name="Reshape_1326" type="Const" version="opset1">
			<data element_type="f32" shape="1, 36, 1, 1" offset="24836" size="144"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>36</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="48" name="214" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="214, Concat_1325, Reshape_1326"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>36</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>36</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="214">
					<dim>-1</dim>
					<dim>36</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="49" name="Split_1117.0" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14292" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="50" name="Split_1117.1" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14324" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="51" name="211" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="64" size="4"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="211"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="211"/>
			</output>
		</layer>
		<layer id="52" name="212" type="Pad" version="opset1">
			<data pad_mode="constant"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="212, Split_1117"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
				<port id="2" precision="I64">
					<dim>4</dim>
				</port>
				<port id="3" precision="FP32"/>
			</input>
			<output>
				<port id="4" precision="FP32" names="212">
					<dim>-1</dim>
					<dim>36</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="53" name="215" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="215"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>36</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>36</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="215">
					<dim>-1</dim>
					<dim>36</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="54" name="216" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="216"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>36</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="216">
					<dim>-1</dim>
					<dim>36</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="55" name="Reshape_1464" type="Const" version="opset1">
			<data element_type="f32" shape="36, 1, 1, 3, 3" offset="24980" size="1296"/>
			<output>
				<port id="0" precision="FP32">
					<dim>36</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="56" name="GroupConvolution_1530" type="GroupConvolution" version="opset1">
			<data strides="1, 1" pads_begin="1, 1" pads_end="1, 1" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="241, Concat_1549, GroupConvolution_1530, Reshape_1464, Reshape_1550"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>36</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>36</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="241">
					<dim>-1</dim>
					<dim>36</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="57" name="backbone1.6.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="42, 36, 1, 1" offset="26276" size="6048"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone1.6.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone1.6.convs.1.weight">
					<dim>42</dim>
					<dim>36</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="58" name="Convolution_1578" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_1578"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>36</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>42</dim>
					<dim>36</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="59" name="Reshape_1598" type="Const" version="opset1">
			<data element_type="f32" shape="1, 42, 1, 1" offset="32324" size="168"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>42</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="60" name="242" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="242, Concat_1597, Reshape_1598"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>42</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="242">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="61" name="Split_1389.0" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14292" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="62" name="Split_1389.1" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="32492" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="63" name="239" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="64" size="4"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="239"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="239"/>
			</output>
		</layer>
		<layer id="64" name="240" type="Pad" version="opset1">
			<data pad_mode="constant"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="240, Split_1389"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>36</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
				<port id="2" precision="I64">
					<dim>4</dim>
				</port>
				<port id="3" precision="FP32"/>
			</input>
			<output>
				<port id="4" precision="FP32" names="240">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="65" name="243" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="243"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="243">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="66" name="244" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="244"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="244">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="67" name="Reshape_1838" type="Const" version="opset1">
			<data element_type="f32" shape="42, 1, 1, 3, 3" offset="32524" size="1512"/>
			<output>
				<port id="0" precision="FP32">
					<dim>42</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="68" name="GroupConvolution_1904" type="GroupConvolution" version="opset1">
			<data strides="2, 2" pads_begin="0, 0" pads_end="2, 2" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="268, 294, Concat_1923, GroupConvolution_1904, Reshape_1838, Reshape_1924, Split_1661"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>42</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="294">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="69" name="backbone1.7.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="48, 42, 1, 1" offset="34036" size="8064"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone1.7.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone1.7.convs.1.weight">
					<dim>48</dim>
					<dim>42</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="70" name="Convolution_1952" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_1952"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>48</dim>
					<dim>42</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>48</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="71" name="Reshape_1972" type="Const" version="opset1">
			<data element_type="f32" shape="1, 48, 1, 1" offset="42100" size="192"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>48</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="72" name="295" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="295, Concat_1971, Reshape_1972"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>48</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>48</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="295">
					<dim>-1</dim>
					<dim>48</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="73" name="269" type="MaxPool" version="opset8">
			<data strides="2, 2" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" kernel="2, 2" rounding_type="floor" auto_pad="explicit" index_element_type="i64" axis="0"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="269"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>32</dim>
					<dim>32</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="269">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="2" precision="I64">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="74" name="Split_1763.0" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14292" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="75" name="Split_1763.1" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="32492" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="76" name="292" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="64" size="4"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="292"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="292"/>
			</output>
		</layer>
		<layer id="77" name="293" type="Pad" version="opset1">
			<data pad_mode="constant"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="293, Split_1763"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>42</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
				<port id="2" precision="I64">
					<dim>4</dim>
				</port>
				<port id="3" precision="FP32"/>
			</input>
			<output>
				<port id="4" precision="FP32" names="293">
					<dim>-1</dim>
					<dim>48</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="78" name="296" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="296"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>48</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>48</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="296">
					<dim>-1</dim>
					<dim>48</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="79" name="297" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="297"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>48</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="297">
					<dim>-1</dim>
					<dim>48</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="80" name="Reshape_2110" type="Const" version="opset1">
			<data element_type="f32" shape="48, 1, 1, 3, 3" offset="42292" size="1728"/>
			<output>
				<port id="0" precision="FP32">
					<dim>48</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="81" name="GroupConvolution_2176" type="GroupConvolution" version="opset1">
			<data strides="1, 1" pads_begin="1, 1" pads_end="1, 1" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="322, Concat_2195, GroupConvolution_2176, Reshape_2110, Reshape_2196"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>48</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>48</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="322">
					<dim>-1</dim>
					<dim>48</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="82" name="backbone1.8.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="56, 48, 1, 1" offset="44020" size="10752"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone1.8.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone1.8.convs.1.weight">
					<dim>56</dim>
					<dim>48</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="83" name="Convolution_2224" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_2224"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>48</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>56</dim>
					<dim>48</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>56</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="84" name="Reshape_2244" type="Const" version="opset1">
			<data element_type="f32" shape="1, 56, 1, 1" offset="54772" size="224"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>56</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="85" name="323" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="323, Concat_2243, Reshape_2244"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>56</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>56</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="323">
					<dim>-1</dim>
					<dim>56</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="86" name="Split_2035.0" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14292" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="87" name="Split_2035.1" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="54996" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="88" name="320" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="64" size="4"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="320"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="320"/>
			</output>
		</layer>
		<layer id="89" name="321" type="Pad" version="opset1">
			<data pad_mode="constant"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="321, Split_2035"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>48</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
				<port id="2" precision="I64">
					<dim>4</dim>
				</port>
				<port id="3" precision="FP32"/>
			</input>
			<output>
				<port id="4" precision="FP32" names="321">
					<dim>-1</dim>
					<dim>56</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="90" name="324" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="324"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>56</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>56</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="324">
					<dim>-1</dim>
					<dim>56</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="91" name="325" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="325"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>56</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="325">
					<dim>-1</dim>
					<dim>56</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="92" name="Reshape_2382" type="Const" version="opset1">
			<data element_type="f32" shape="56, 1, 1, 3, 3" offset="55028" size="2016"/>
			<output>
				<port id="0" precision="FP32">
					<dim>56</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="93" name="GroupConvolution_2448" type="GroupConvolution" version="opset1">
			<data strides="1, 1" pads_begin="1, 1" pads_end="1, 1" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="350, Concat_2467, GroupConvolution_2448, Reshape_2382, Reshape_2468"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>56</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>56</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="350">
					<dim>-1</dim>
					<dim>56</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="94" name="backbone1.9.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="64, 56, 1, 1" offset="57044" size="14336"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone1.9.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone1.9.convs.1.weight">
					<dim>64</dim>
					<dim>56</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="95" name="Convolution_2496" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_2496"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>56</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>64</dim>
					<dim>56</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="96" name="Reshape_2516" type="Const" version="opset1">
			<data element_type="f32" shape="1, 64, 1, 1" offset="71380" size="256"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="97" name="351" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="351, Concat_2515, Reshape_2516"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="351">
					<dim>-1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="98" name="Split_2307.0" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14292" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="99" name="Split_2307.1" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="54996" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="100" name="348" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="64" size="4"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="348"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="348"/>
			</output>
		</layer>
		<layer id="101" name="349" type="Pad" version="opset1">
			<data pad_mode="constant"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="349, Split_2307"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>56</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
				<port id="2" precision="I64">
					<dim>4</dim>
				</port>
				<port id="3" precision="FP32"/>
			</input>
			<output>
				<port id="4" precision="FP32" names="349">
					<dim>-1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="102" name="352" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="352"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="352">
					<dim>-1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="103" name="353" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="353"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="353">
					<dim>-1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="104" name="Reshape_2654" type="Const" version="opset1">
			<data element_type="f32" shape="64, 1, 1, 3, 3" offset="71636" size="2304"/>
			<output>
				<port id="0" precision="FP32">
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="105" name="GroupConvolution_2720" type="GroupConvolution" version="opset1">
			<data strides="1, 1" pads_begin="1, 1" pads_end="1, 1" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="378, Concat_2739, GroupConvolution_2720, Reshape_2654, Reshape_2740"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="378">
					<dim>-1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="106" name="backbone1.10.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="72, 64, 1, 1" offset="73940" size="18432"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone1.10.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone1.10.convs.1.weight">
					<dim>72</dim>
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="107" name="Convolution_2768" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_2768"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>72</dim>
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>72</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="108" name="Reshape_2788" type="Const" version="opset1">
			<data element_type="f32" shape="1, 72, 1, 1" offset="92372" size="288"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="109" name="379" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="379, Concat_2787, Reshape_2788"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>72</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="379">
					<dim>-1</dim>
					<dim>72</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="110" name="Split_2579.0" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14292" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="111" name="Split_2579.1" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="54996" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="112" name="376" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="64" size="4"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="376"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="376"/>
			</output>
		</layer>
		<layer id="113" name="377" type="Pad" version="opset1">
			<data pad_mode="constant"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="377, Split_2579"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
				<port id="2" precision="I64">
					<dim>4</dim>
				</port>
				<port id="3" precision="FP32"/>
			</input>
			<output>
				<port id="4" precision="FP32" names="377">
					<dim>-1</dim>
					<dim>72</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="114" name="380" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="380"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>72</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>72</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="380">
					<dim>-1</dim>
					<dim>72</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="115" name="381" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="381"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>72</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="381">
					<dim>-1</dim>
					<dim>72</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="116" name="Reshape_2926" type="Const" version="opset1">
			<data element_type="f32" shape="72, 1, 1, 3, 3" offset="92660" size="2592"/>
			<output>
				<port id="0" precision="FP32">
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="117" name="GroupConvolution_2992" type="GroupConvolution" version="opset1">
			<data strides="1, 1" pads_begin="1, 1" pads_end="1, 1" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="406, Concat_3011, GroupConvolution_2992, Reshape_2926, Reshape_3012"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>72</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="406">
					<dim>-1</dim>
					<dim>72</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="118" name="backbone1.11.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="80, 72, 1, 1" offset="95252" size="23040"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone1.11.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone1.11.convs.1.weight">
					<dim>80</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="119" name="Convolution_3040" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_3040"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>72</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>80</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>80</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="120" name="Reshape_3060" type="Const" version="opset1">
			<data element_type="f32" shape="1, 80, 1, 1" offset="118292" size="320"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="121" name="407" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="407, Concat_3059, Reshape_3060"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>80</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="407">
					<dim>-1</dim>
					<dim>80</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="122" name="Split_2851.0" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14292" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="123" name="Split_2851.1" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="54996" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="124" name="404" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="64" size="4"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="404"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="404"/>
			</output>
		</layer>
		<layer id="125" name="405" type="Pad" version="opset1">
			<data pad_mode="constant"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="405, Split_2851"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>72</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
				<port id="2" precision="I64">
					<dim>4</dim>
				</port>
				<port id="3" precision="FP32"/>
			</input>
			<output>
				<port id="4" precision="FP32" names="405">
					<dim>-1</dim>
					<dim>80</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="126" name="408" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="408"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>80</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>80</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="408">
					<dim>-1</dim>
					<dim>80</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="127" name="409" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="409"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>80</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="409">
					<dim>-1</dim>
					<dim>80</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="128" name="Reshape_3198" type="Const" version="opset1">
			<data element_type="f32" shape="80, 1, 1, 3, 3" offset="118612" size="2880"/>
			<output>
				<port id="0" precision="FP32">
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="129" name="GroupConvolution_3264" type="GroupConvolution" version="opset1">
			<data strides="1, 1" pads_begin="1, 1" pads_end="1, 1" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="434, Concat_3283, GroupConvolution_3264, Reshape_3198, Reshape_3284"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>80</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="434">
					<dim>-1</dim>
					<dim>80</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="130" name="backbone1.12.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="88, 80, 1, 1" offset="121492" size="28160"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone1.12.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone1.12.convs.1.weight">
					<dim>88</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="131" name="Convolution_3312" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_3312"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>80</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>88</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="132" name="Reshape_3332" type="Const" version="opset1">
			<data element_type="f32" shape="1, 88, 1, 1" offset="149652" size="352"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>88</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="133" name="435" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="435, Concat_3331, Reshape_3332"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>88</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="435">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="134" name="Split_3123.0" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14292" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="135" name="Split_3123.1" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="54996" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="136" name="432" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="64" size="4"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="432"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="432"/>
			</output>
		</layer>
		<layer id="137" name="433" type="Pad" version="opset1">
			<data pad_mode="constant"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="433, Split_3123"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>80</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
				<port id="2" precision="I64">
					<dim>4</dim>
				</port>
				<port id="3" precision="FP32"/>
			</input>
			<output>
				<port id="4" precision="FP32" names="433">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="138" name="436" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="436"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="436">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="139" name="437" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="437"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="437">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="140" name="classifier_8.weight" type="Const" version="opset1">
			<data element_type="f32" shape="2, 88, 1, 1" offset="150004" size="704"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="classifier_8.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="classifier_8.weight">
					<dim>2</dim>
					<dim>88</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="141" name="Convolution_4420" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_4420"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>2</dim>
					<dim>88</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>2</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="142" name="Reshape_4440" type="Const" version="opset1">
			<data element_type="f32" shape="1, 2, 1, 1" offset="150708" size="8"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>2</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="143" name="507" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="507, Concat_4439, Reshape_4440"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>2</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>2</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="507">
					<dim>-1</dim>
					<dim>2</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="144" name="Constant_4468" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="150716" size="32"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Constant_4468"/>
			</rt_info>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="145" name="508" type="Transpose" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="508"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>2</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="508">
					<dim>-1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="146" name="99" type="ShapeOf" version="opset3">
			<data output_type="i64"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="99"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>3</dim>
					<dim>131</dim>
					<dim>131</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="I64" names="99">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="147" name="Constant_10203" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="150748" size="8"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="100, 101, 511, Constant_216, Constant_4470"/>
			</rt_info>
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="148" name="Constant_216" type="Const" version="opset1">
			<data element_type="i64" shape="" offset="150748" size="8"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Constant_216"/>
			</rt_info>
			<output>
				<port id="0" precision="I64"/>
			</output>
		</layer>
		<layer id="149" name="101" type="Gather" version="opset8">
			<data batch_dims="0"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="100, 101, 511, Constant_216, Constant_4470"/>
			</rt_info>
			<input>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64"/>
			</input>
			<output>
				<port id="3" precision="I64" names="511">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="150" name="620" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="150756" size="8"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="620"/>
			</rt_info>
			<output>
				<port id="0" precision="I64" names="620">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="151" name="621" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="150764" size="8"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="621"/>
			</rt_info>
			<output>
				<port id="0" precision="I64" names="621">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="152" name="514" type="Concat" version="opset1">
			<data axis="0"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="514, 620, 621"/>
			</rt_info>
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="3" precision="I64" names="514">
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="153" name="515" type="Reshape" version="opset1">
			<data special_zero="true"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="515"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>2</dim>
				</port>
				<port id="1" precision="I64">
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="515">
					<dim>-1</dim>
					<dim>-1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="154" name="Reshape_3572" type="Const" version="opset1">
			<data element_type="f32" shape="88, 1, 1, 3, 3" offset="150772" size="3168"/>
			<output>
				<port id="0" precision="FP32">
					<dim>88</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="155" name="GroupConvolution_3638" type="GroupConvolution" version="opset1">
			<data strides="2, 2" pads_begin="0, 0" pads_end="2, 2" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="461, 487, Concat_3657, GroupConvolution_3638, Reshape_3572, Reshape_3658, Split_3395"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>88</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="487">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="156" name="backbone2.0.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="96, 88, 1, 1" offset="153940" size="33792"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone2.0.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone2.0.convs.1.weight">
					<dim>96</dim>
					<dim>88</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="157" name="Convolution_3686" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_3686"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>96</dim>
					<dim>88</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="158" name="Reshape_3706" type="Const" version="opset1">
			<data element_type="f32" shape="1, 96, 1, 1" offset="187732" size="384"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="159" name="488" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="488, Concat_3705, Reshape_3706"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="488">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="160" name="462" type="MaxPool" version="opset8">
			<data strides="2, 2" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" kernel="2, 2" rounding_type="floor" auto_pad="explicit" index_element_type="i64" axis="0"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="462"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="462">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="2" precision="I64">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="161" name="Split_3497.0" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="14292" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="162" name="Split_3497.1" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="54996" size="32"/>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="163" name="485" type="Const" version="opset1">
			<data element_type="f32" shape="" offset="64" size="4"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="485"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="485"/>
			</output>
		</layer>
		<layer id="164" name="486" type="Pad" version="opset1">
			<data pad_mode="constant"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="486, Split_3497"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
				<port id="2" precision="I64">
					<dim>4</dim>
				</port>
				<port id="3" precision="FP32"/>
			</input>
			<output>
				<port id="4" precision="FP32" names="486">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="165" name="489" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="489"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="489">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="166" name="490" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="490"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="490">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="167" name="Reshape_3743" type="Const" version="opset1">
			<data element_type="f32" shape="96, 1, 1, 3, 3" offset="188116" size="3456"/>
			<output>
				<port id="0" precision="FP32">
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="168" name="GroupConvolution_3809" type="GroupConvolution" version="opset1">
			<data strides="1, 1" pads_begin="1, 1" pads_end="1, 1" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="491, Concat_3828, GroupConvolution_3809, Reshape_3743, Reshape_3829"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="491">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="169" name="backbone2.1.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="96, 96, 1, 1" offset="191572" size="36864"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone2.1.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone2.1.convs.1.weight">
					<dim>96</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="170" name="Convolution_3857" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_3857"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>96</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="171" name="Reshape_3877" type="Const" version="opset1">
			<data element_type="f32" shape="1, 96, 1, 1" offset="228436" size="384"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="172" name="492" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="492, Concat_3876, Reshape_3877"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="492">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="173" name="493" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="493"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="493">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="174" name="494" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="494"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="494">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="175" name="Reshape_3914" type="Const" version="opset1">
			<data element_type="f32" shape="96, 1, 1, 3, 3" offset="228820" size="3456"/>
			<output>
				<port id="0" precision="FP32">
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="176" name="GroupConvolution_3980" type="GroupConvolution" version="opset1">
			<data strides="1, 1" pads_begin="1, 1" pads_end="1, 1" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="495, Concat_3999, GroupConvolution_3980, Reshape_3914, Reshape_4000"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="495">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="177" name="backbone2.2.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="96, 96, 1, 1" offset="232276" size="36864"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone2.2.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone2.2.convs.1.weight">
					<dim>96</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="178" name="Convolution_4028" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_4028"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>96</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="179" name="Reshape_4048" type="Const" version="opset1">
			<data element_type="f32" shape="1, 96, 1, 1" offset="269140" size="384"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="180" name="496" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="496, Concat_4047, Reshape_4048"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="496">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="181" name="497" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="497"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="497">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="182" name="498" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="498"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="498">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="183" name="Reshape_4085" type="Const" version="opset1">
			<data element_type="f32" shape="96, 1, 1, 3, 3" offset="269524" size="3456"/>
			<output>
				<port id="0" precision="FP32">
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="184" name="GroupConvolution_4151" type="GroupConvolution" version="opset1">
			<data strides="1, 1" pads_begin="1, 1" pads_end="1, 1" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="499, Concat_4170, GroupConvolution_4151, Reshape_4085, Reshape_4171"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="499">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="185" name="backbone2.3.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="96, 96, 1, 1" offset="272980" size="36864"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone2.3.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone2.3.convs.1.weight">
					<dim>96</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="186" name="Convolution_4199" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_4199"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>96</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="187" name="Reshape_4219" type="Const" version="opset1">
			<data element_type="f32" shape="1, 96, 1, 1" offset="309844" size="384"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="188" name="500" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="500, Concat_4218, Reshape_4219"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="500">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="189" name="501" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="501"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="501">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="190" name="502" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="502"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="502">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="191" name="Reshape_4256" type="Const" version="opset1">
			<data element_type="f32" shape="96, 1, 1, 3, 3" offset="310228" size="3456"/>
			<output>
				<port id="0" precision="FP32">
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="192" name="GroupConvolution_4322" type="GroupConvolution" version="opset1">
			<data strides="1, 1" pads_begin="1, 1" pads_end="1, 1" dilations="1, 1" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="503, Concat_4341, GroupConvolution_4322, Reshape_4256, Reshape_4342"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="503">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="193" name="backbone2.4.convs.1.weight" type="Const" version="opset1">
			<data element_type="f32" shape="96, 96, 1, 1" offset="313684" size="36864"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="backbone2.4.convs.1.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="backbone2.4.convs.1.weight">
					<dim>96</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="194" name="Convolution_4370" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_4370"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>96</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="195" name="Reshape_4390" type="Const" version="opset1">
			<data element_type="f32" shape="1, 96, 1, 1" offset="350548" size="384"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="196" name="504" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="504, Concat_4389, Reshape_4390"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="504">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="197" name="505" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="505"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="505">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="198" name="506" type="ReLU" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="506"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="FP32" names="506">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="199" name="classifier_16.weight" type="Const" version="opset1">
			<data element_type="f32" shape="6, 96, 1, 1" offset="350932" size="2304"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="classifier_16.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="classifier_16.weight">
					<dim>6</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="200" name="Convolution_4543" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_4543"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>6</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>6</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="201" name="Reshape_4563" type="Const" version="opset1">
			<data element_type="f32" shape="1, 6, 1, 1" offset="353236" size="24"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>6</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="202" name="516" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="516, Concat_4562, Reshape_4563"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>6</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>6</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="516">
					<dim>-1</dim>
					<dim>6</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="203" name="Constant_4591" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="150716" size="32"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Constant_4591"/>
			</rt_info>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="204" name="517" type="Transpose" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="517"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>6</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="517">
					<dim>-1</dim>
					<dim>8</dim>
					<dim>8</dim>
					<dim>6</dim>
				</port>
			</output>
		</layer>
		<layer id="205" name="Constant_10212" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="150748" size="8"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="100, 101, 520, Constant_216, Constant_4593"/>
			</rt_info>
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="206" name="1010" type="Gather" version="opset8">
			<data batch_dims="0"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="100, 101, 520, Constant_216, Constant_4593"/>
			</rt_info>
			<input>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64"/>
			</input>
			<output>
				<port id="3" precision="I64" names="520">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="207" name="622" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="150756" size="8"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="622"/>
			</rt_info>
			<output>
				<port id="0" precision="I64" names="622">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="208" name="623" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="150764" size="8"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="623"/>
			</rt_info>
			<output>
				<port id="0" precision="I64" names="623">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="209" name="523" type="Concat" version="opset1">
			<data axis="0"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="523, 622, 623"/>
			</rt_info>
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="3" precision="I64" names="523">
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="210" name="524" type="Reshape" version="opset1">
			<data special_zero="true"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="524"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>8</dim>
					<dim>8</dim>
					<dim>6</dim>
				</port>
				<port id="1" precision="I64">
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="524">
					<dim>-1</dim>
					<dim>-1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="211" name="525" type="Concat" version="opset1">
			<data axis="1"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="525"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>-1</dim>
					<dim>1</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>-1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="525">
					<dim>-1</dim>
					<dim>-1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="213" name="regressor_8.weight" type="Const" version="opset1">
			<data element_type="f32" shape="32, 88, 1, 1" offset="353260" size="11264"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="regressor_8.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="regressor_8.weight">
					<dim>32</dim>
					<dim>88</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="214" name="Convolution_4667" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_4667"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>88</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>32</dim>
					<dim>88</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="215" name="Reshape_4687" type="Const" version="opset1">
			<data element_type="f32" shape="1, 32, 1, 1" offset="364524" size="128"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>32</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="216" name="526" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="526, Concat_4686, Reshape_4687"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>32</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="526">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="217" name="Constant_4715" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="150716" size="32"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Constant_4715"/>
			</rt_info>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="218" name="527" type="Transpose" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="527"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>32</dim>
					<dim>16</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="527">
					<dim>-1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>32</dim>
				</port>
			</output>
		</layer>
		<layer id="219" name="Constant_10221" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="150748" size="8"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="100, 101, 530, Constant_216, Constant_4717"/>
			</rt_info>
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="220" name="1011" type="Gather" version="opset8">
			<data batch_dims="0"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="100, 101, 530, Constant_216, Constant_4717"/>
			</rt_info>
			<input>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64"/>
			</input>
			<output>
				<port id="3" precision="I64" names="530">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="221" name="624" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="150756" size="8"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="624"/>
			</rt_info>
			<output>
				<port id="0" precision="I64" names="624">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="222" name="625" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="364652" size="8"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="625"/>
			</rt_info>
			<output>
				<port id="0" precision="I64" names="625">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="223" name="533" type="Concat" version="opset1">
			<data axis="0"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="533, 624, 625"/>
			</rt_info>
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="3" precision="I64" names="533">
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="224" name="534" type="Reshape" version="opset1">
			<data special_zero="true"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="534"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>32</dim>
				</port>
				<port id="1" precision="I64">
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="534">
					<dim>-1</dim>
					<dim>-1</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="225" name="regressor_16.weight" type="Const" version="opset1">
			<data element_type="f32" shape="96, 96, 1, 1" offset="364660" size="36864"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="regressor_16.weight"/>
			</rt_info>
			<output>
				<port id="0" precision="FP32" names="regressor_16.weight">
					<dim>96</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="226" name="Convolution_4796" type="Convolution" version="opset1">
			<data strides="1, 1" dilations="1, 1" pads_begin="0, 0" pads_end="0, 0" auto_pad="explicit"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Convolution_4796"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>96</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="227" name="Reshape_4816" type="Const" version="opset1">
			<data element_type="f32" shape="1, 96, 1, 1" offset="401524" size="384"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="228" name="535" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="535, Concat_4815, Reshape_4816"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>1</dim>
					<dim>96</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="535">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
			</output>
		</layer>
		<layer id="229" name="Constant_4844" type="Const" version="opset1">
			<data element_type="i64" shape="4" offset="150716" size="32"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="Constant_4844"/>
			</rt_info>
			<output>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
			</output>
		</layer>
		<layer id="230" name="536" type="Transpose" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="536"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>96</dim>
					<dim>8</dim>
					<dim>8</dim>
				</port>
				<port id="1" precision="I64">
					<dim>4</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="536">
					<dim>-1</dim>
					<dim>8</dim>
					<dim>8</dim>
					<dim>96</dim>
				</port>
			</output>
		</layer>
		<layer id="231" name="Constant_10230" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="150748" size="8"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="100, 101, 539, Constant_216, Constant_4846"/>
			</rt_info>
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="232" name="1012" type="Gather" version="opset8">
			<data batch_dims="0"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="100, 101, 539, Constant_216, Constant_4846"/>
			</rt_info>
			<input>
				<port id="0" precision="I64">
					<dim>4</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64"/>
			</input>
			<output>
				<port id="3" precision="I64" names="539">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="233" name="626" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="150756" size="8"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="626"/>
			</rt_info>
			<output>
				<port id="0" precision="I64" names="626">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="234" name="627" type="Const" version="opset1">
			<data element_type="i64" shape="1" offset="364652" size="8"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="627"/>
			</rt_info>
			<output>
				<port id="0" precision="I64" names="627">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="235" name="542" type="Concat" version="opset1">
			<data axis="0"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="542, 626, 627"/>
			</rt_info>
			<input>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
				<port id="1" precision="I64">
					<dim>1</dim>
				</port>
				<port id="2" precision="I64">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="3" precision="I64" names="542">
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="236" name="543" type="Reshape" version="opset1">
			<data special_zero="true"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="543"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>8</dim>
					<dim>8</dim>
					<dim>96</dim>
				</port>
				<port id="1" precision="I64">
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="543">
					<dim>-1</dim>
					<dim>-1</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="237" name="output" type="Concat" version="opset1">
			<data axis="1"/>
			<rt_info>
				<attribute name="fused_names" version="0" value="output"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>-1</dim>
					<dim>16</dim>
				</port>
				<port id="1" precision="FP32">
					<dim>-1</dim>
					<dim>-1</dim>
					<dim>16</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32" names="output">
					<dim>-1</dim>
					<dim>-1</dim>
					<dim>16</dim>
				</port>
			</output>
		</layer>
		<layer id="238" name="output/sink_port_0" type="Result" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="output/sink_port_0"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>-1</dim>
					<dim>16</dim>
				</port>
			</input>
		</layer>
		<layer id="212" name="525/sink_port_0" type="Result" version="opset1">
			<rt_info>
				<attribute name="fused_names" version="0" value="525/sink_port_0"/>
			</rt_info>
			<input>
				<port id="0" precision="FP32">
					<dim>-1</dim>
					<dim>-1</dim>
					<dim>1</dim>
				</port>
			</input>
		</layer>
	</layers>
	<edges>
		<edge from-layer="0" from-port="0" to-layer="4" to-port="0"/>
		<edge from-layer="1" from-port="0" to-layer="4" to-port="1"/>
		<edge from-layer="2" from-port="0" to-layer="4" to-port="2"/>
		<edge from-layer="3" from-port="0" to-layer="4" to-port="3"/>
		<edge from-layer="4" from-port="4" to-layer="146" to-port="0"/>
		<edge from-layer="4" from-port="4" to-layer="6" to-port="0"/>
		<edge from-layer="5" from-port="0" to-layer="6" to-port="1"/>
		<edge from-layer="6" from-port="2" to-layer="8" to-port="0"/>
		<edge from-layer="7" from-port="0" to-layer="8" to-port="1"/>
		<edge from-layer="8" from-port="2" to-layer="9" to-port="0"/>
		<edge from-layer="9" from-port="1" to-layer="11" to-port="0"/>
		<edge from-layer="9" from-port="1" to-layer="16" to-port="1"/>
		<edge from-layer="10" from-port="0" to-layer="11" to-port="1"/>
		<edge from-layer="11" from-port="2" to-layer="13" to-port="0"/>
		<edge from-layer="12" from-port="0" to-layer="13" to-port="1"/>
		<edge from-layer="13" from-port="2" to-layer="15" to-port="0"/>
		<edge from-layer="14" from-port="0" to-layer="15" to-port="1"/>
		<edge from-layer="15" from-port="2" to-layer="16" to-port="0"/>
		<edge from-layer="16" from-port="2" to-layer="17" to-port="0"/>
		<edge from-layer="17" from-port="1" to-layer="19" to-port="0"/>
		<edge from-layer="17" from-port="1" to-layer="27" to-port="0"/>
		<edge from-layer="18" from-port="0" to-layer="19" to-port="1"/>
		<edge from-layer="19" from-port="2" to-layer="21" to-port="0"/>
		<edge from-layer="20" from-port="0" to-layer="21" to-port="1"/>
		<edge from-layer="21" from-port="2" to-layer="23" to-port="0"/>
		<edge from-layer="22" from-port="0" to-layer="23" to-port="1"/>
		<edge from-layer="23" from-port="2" to-layer="28" to-port="0"/>
		<edge from-layer="24" from-port="0" to-layer="27" to-port="1"/>
		<edge from-layer="25" from-port="0" to-layer="27" to-port="2"/>
		<edge from-layer="26" from-port="0" to-layer="27" to-port="3"/>
		<edge from-layer="27" from-port="4" to-layer="28" to-port="1"/>
		<edge from-layer="28" from-port="2" to-layer="29" to-port="0"/>
		<edge from-layer="29" from-port="1" to-layer="31" to-port="0"/>
		<edge from-layer="29" from-port="1" to-layer="36" to-port="0"/>
		<edge from-layer="30" from-port="0" to-layer="31" to-port="1"/>
		<edge from-layer="31" from-port="2" to-layer="33" to-port="0"/>
		<edge from-layer="32" from-port="0" to-layer="33" to-port="1"/>
		<edge from-layer="33" from-port="2" to-layer="35" to-port="0"/>
		<edge from-layer="34" from-port="0" to-layer="35" to-port="1"/>
		<edge from-layer="35" from-port="2" to-layer="41" to-port="0"/>
		<edge from-layer="36" from-port="1" to-layer="40" to-port="0"/>
		<edge from-layer="37" from-port="0" to-layer="40" to-port="1"/>
		<edge from-layer="38" from-port="0" to-layer="40" to-port="2"/>
		<edge from-layer="39" from-port="0" to-layer="40" to-port="3"/>
		<edge from-layer="40" from-port="4" to-layer="41" to-port="1"/>
		<edge from-layer="41" from-port="2" to-layer="42" to-port="0"/>
		<edge from-layer="42" from-port="1" to-layer="44" to-port="0"/>
		<edge from-layer="42" from-port="1" to-layer="52" to-port="0"/>
		<edge from-layer="43" from-port="0" to-layer="44" to-port="1"/>
		<edge from-layer="44" from-port="2" to-layer="46" to-port="0"/>
		<edge from-layer="45" from-port="0" to-layer="46" to-port="1"/>
		<edge from-layer="46" from-port="2" to-layer="48" to-port="0"/>
		<edge from-layer="47" from-port="0" to-layer="48" to-port="1"/>
		<edge from-layer="48" from-port="2" to-layer="53" to-port="0"/>
		<edge from-layer="49" from-port="0" to-layer="52" to-port="1"/>
		<edge from-layer="50" from-port="0" to-layer="52" to-port="2"/>
		<edge from-layer="51" from-port="0" to-layer="52" to-port="3"/>
		<edge from-layer="52" from-port="4" to-layer="53" to-port="1"/>
		<edge from-layer="53" from-port="2" to-layer="54" to-port="0"/>
		<edge from-layer="54" from-port="1" to-layer="56" to-port="0"/>
		<edge from-layer="54" from-port="1" to-layer="64" to-port="0"/>
		<edge from-layer="55" from-port="0" to-layer="56" to-port="1"/>
		<edge from-layer="56" from-port="2" to-layer="58" to-port="0"/>
		<edge from-layer="57" from-port="0" to-layer="58" to-port="1"/>
		<edge from-layer="58" from-port="2" to-layer="60" to-port="0"/>
		<edge from-layer="59" from-port="0" to-layer="60" to-port="1"/>
		<edge from-layer="60" from-port="2" to-layer="65" to-port="0"/>
		<edge from-layer="61" from-port="0" to-layer="64" to-port="1"/>
		<edge from-layer="62" from-port="0" to-layer="64" to-port="2"/>
		<edge from-layer="63" from-port="0" to-layer="64" to-port="3"/>
		<edge from-layer="64" from-port="4" to-layer="65" to-port="1"/>
		<edge from-layer="65" from-port="2" to-layer="66" to-port="0"/>
		<edge from-layer="66" from-port="1" to-layer="68" to-port="0"/>
		<edge from-layer="66" from-port="1" to-layer="73" to-port="0"/>
		<edge from-layer="67" from-port="0" to-layer="68" to-port="1"/>
		<edge from-layer="68" from-port="2" to-layer="70" to-port="0"/>
		<edge from-layer="69" from-port="0" to-layer="70" to-port="1"/>
		<edge from-layer="70" from-port="2" to-layer="72" to-port="0"/>
		<edge from-layer="71" from-port="0" to-layer="72" to-port="1"/>
		<edge from-layer="72" from-port="2" to-layer="78" to-port="0"/>
		<edge from-layer="73" from-port="1" to-layer="77" to-port="0"/>
		<edge from-layer="74" from-port="0" to-layer="77" to-port="1"/>
		<edge from-layer="75" from-port="0" to-layer="77" to-port="2"/>
		<edge from-layer="76" from-port="0" to-layer="77" to-port="3"/>
		<edge from-layer="77" from-port="4" to-layer="78" to-port="1"/>
		<edge from-layer="78" from-port="2" to-layer="79" to-port="0"/>
		<edge from-layer="79" from-port="1" to-layer="81" to-port="0"/>
		<edge from-layer="79" from-port="1" to-layer="89" to-port="0"/>
		<edge from-layer="80" from-port="0" to-layer="81" to-port="1"/>
		<edge from-layer="81" from-port="2" to-layer="83" to-port="0"/>
		<edge from-layer="82" from-port="0" to-layer="83" to-port="1"/>
		<edge from-layer="83" from-port="2" to-layer="85" to-port="0"/>
		<edge from-layer="84" from-port="0" to-layer="85" to-port="1"/>
		<edge from-layer="85" from-port="2" to-layer="90" to-port="0"/>
		<edge from-layer="86" from-port="0" to-layer="89" to-port="1"/>
		<edge from-layer="87" from-port="0" to-layer="89" to-port="2"/>
		<edge from-layer="88" from-port="0" to-layer="89" to-port="3"/>
		<edge from-layer="89" from-port="4" to-layer="90" to-port="1"/>
		<edge from-layer="90" from-port="2" to-layer="91" to-port="0"/>
		<edge from-layer="91" from-port="1" to-layer="93" to-port="0"/>
		<edge from-layer="91" from-port="1" to-layer="101" to-port="0"/>
		<edge from-layer="92" from-port="0" to-layer="93" to-port="1"/>
		<edge from-layer="93" from-port="2" to-layer="95" to-port="0"/>
		<edge from-layer="94" from-port="0" to-layer="95" to-port="1"/>
		<edge from-layer="95" from-port="2" to-layer="97" to-port="0"/>
		<edge from-layer="96" from-port="0" to-layer="97" to-port="1"/>
		<edge from-layer="97" from-port="2" to-layer="102" to-port="0"/>
		<edge from-layer="98" from-port="0" to-layer="101" to-port="1"/>
		<edge from-layer="99" from-port="0" to-layer="101" to-port="2"/>
		<edge from-layer="100" from-port="0" to-layer="101" to-port="3"/>
		<edge from-layer="101" from-port="4" to-layer="102" to-port="1"/>
		<edge from-layer="102" from-port="2" to-layer="103" to-port="0"/>
		<edge from-layer="103" from-port="1" to-layer="105" to-port="0"/>
		<edge from-layer="103" from-port="1" to-layer="113" to-port="0"/>
		<edge from-layer="104" from-port="0" to-layer="105" to-port="1"/>
		<edge from-layer="105" from-port="2" to-layer="107" to-port="0"/>
		<edge from-layer="106" from-port="0" to-layer="107" to-port="1"/>
		<edge from-layer="107" from-port="2" to-layer="109" to-port="0"/>
		<edge from-layer="108" from-port="0" to-layer="109" to-port="1"/>
		<edge from-layer="109" from-port="2" to-layer="114" to-port="0"/>
		<edge from-layer="110" from-port="0" to-layer="113" to-port="1"/>
		<edge from-layer="111" from-port="0" to-layer="113" to-port="2"/>
		<edge from-layer="112" from-port="0" to-layer="113" to-port="3"/>
		<edge from-layer="113" from-port="4" to-layer="114" to-port="1"/>
		<edge from-layer="114" from-port="2" to-layer="115" to-port="0"/>
		<edge from-layer="115" from-port="1" to-layer="117" to-port="0"/>
		<edge from-layer="115" from-port="1" to-layer="125" to-port="0"/>
		<edge from-layer="116" from-port="0" to-layer="117" to-port="1"/>
		<edge from-layer="117" from-port="2" to-layer="119" to-port="0"/>
		<edge from-layer="118" from-port="0" to-layer="119" to-port="1"/>
		<edge from-layer="119" from-port="2" to-layer="121" to-port="0"/>
		<edge from-layer="120" from-port="0" to-layer="121" to-port="1"/>
		<edge from-layer="121" from-port="2" to-layer="126" to-port="0"/>
		<edge from-layer="122" from-port="0" to-layer="125" to-port="1"/>
		<edge from-layer="123" from-port="0" to-layer="125" to-port="2"/>
		<edge from-layer="124" from-port="0" to-layer="125" to-port="3"/>
		<edge from-layer="125" from-port="4" to-layer="126" to-port="1"/>
		<edge from-layer="126" from-port="2" to-layer="127" to-port="0"/>
		<edge from-layer="127" from-port="1" to-layer="137" to-port="0"/>
		<edge from-layer="127" from-port="1" to-layer="129" to-port="0"/>
		<edge from-layer="128" from-port="0" to-layer="129" to-port="1"/>
		<edge from-layer="129" from-port="2" to-layer="131" to-port="0"/>
		<edge from-layer="130" from-port="0" to-layer="131" to-port="1"/>
		<edge from-layer="131" from-port="2" to-layer="133" to-port="0"/>
		<edge from-layer="132" from-port="0" to-layer="133" to-port="1"/>
		<edge from-layer="133" from-port="2" to-layer="138" to-port="0"/>
		<edge from-layer="134" from-port="0" to-layer="137" to-port="1"/>
		<edge from-layer="135" from-port="0" to-layer="137" to-port="2"/>
		<edge from-layer="136" from-port="0" to-layer="137" to-port="3"/>
		<edge from-layer="137" from-port="4" to-layer="138" to-port="1"/>
		<edge from-layer="138" from-port="2" to-layer="139" to-port="0"/>
		<edge from-layer="139" from-port="1" to-layer="214" to-port="0"/>
		<edge from-layer="139" from-port="1" to-layer="155" to-port="0"/>
		<edge from-layer="139" from-port="1" to-layer="160" to-port="0"/>
		<edge from-layer="139" from-port="1" to-layer="141" to-port="0"/>
		<edge from-layer="140" from-port="0" to-layer="141" to-port="1"/>
		<edge from-layer="141" from-port="2" to-layer="143" to-port="0"/>
		<edge from-layer="142" from-port="0" to-layer="143" to-port="1"/>
		<edge from-layer="143" from-port="2" to-layer="145" to-port="0"/>
		<edge from-layer="144" from-port="0" to-layer="145" to-port="1"/>
		<edge from-layer="145" from-port="2" to-layer="153" to-port="0"/>
		<edge from-layer="146" from-port="1" to-layer="206" to-port="0"/>
		<edge from-layer="146" from-port="1" to-layer="220" to-port="0"/>
		<edge from-layer="146" from-port="1" to-layer="232" to-port="0"/>
		<edge from-layer="146" from-port="1" to-layer="149" to-port="0"/>
		<edge from-layer="147" from-port="0" to-layer="149" to-port="1"/>
		<edge from-layer="148" from-port="0" to-layer="206" to-port="2"/>
		<edge from-layer="148" from-port="0" to-layer="149" to-port="2"/>
		<edge from-layer="148" from-port="0" to-layer="220" to-port="2"/>
		<edge from-layer="148" from-port="0" to-layer="232" to-port="2"/>
		<edge from-layer="149" from-port="3" to-layer="152" to-port="0"/>
		<edge from-layer="150" from-port="0" to-layer="152" to-port="1"/>
		<edge from-layer="151" from-port="0" to-layer="152" to-port="2"/>
		<edge from-layer="152" from-port="3" to-layer="153" to-port="1"/>
		<edge from-layer="153" from-port="2" to-layer="211" to-port="0"/>
		<edge from-layer="154" from-port="0" to-layer="155" to-port="1"/>
		<edge from-layer="155" from-port="2" to-layer="157" to-port="0"/>
		<edge from-layer="156" from-port="0" to-layer="157" to-port="1"/>
		<edge from-layer="157" from-port="2" to-layer="159" to-port="0"/>
		<edge from-layer="158" from-port="0" to-layer="159" to-port="1"/>
		<edge from-layer="159" from-port="2" to-layer="165" to-port="0"/>
		<edge from-layer="160" from-port="1" to-layer="164" to-port="0"/>
		<edge from-layer="161" from-port="0" to-layer="164" to-port="1"/>
		<edge from-layer="162" from-port="0" to-layer="164" to-port="2"/>
		<edge from-layer="163" from-port="0" to-layer="164" to-port="3"/>
		<edge from-layer="164" from-port="4" to-layer="165" to-port="1"/>
		<edge from-layer="165" from-port="2" to-layer="166" to-port="0"/>
		<edge from-layer="166" from-port="1" to-layer="173" to-port="1"/>
		<edge from-layer="166" from-port="1" to-layer="168" to-port="0"/>
		<edge from-layer="167" from-port="0" to-layer="168" to-port="1"/>
		<edge from-layer="168" from-port="2" to-layer="170" to-port="0"/>
		<edge from-layer="169" from-port="0" to-layer="170" to-port="1"/>
		<edge from-layer="170" from-port="2" to-layer="172" to-port="0"/>
		<edge from-layer="171" from-port="0" to-layer="172" to-port="1"/>
		<edge from-layer="172" from-port="2" to-layer="173" to-port="0"/>
		<edge from-layer="173" from-port="2" to-layer="174" to-port="0"/>
		<edge from-layer="174" from-port="1" to-layer="181" to-port="1"/>
		<edge from-layer="174" from-port="1" to-layer="176" to-port="0"/>
		<edge from-layer="175" from-port="0" to-layer="176" to-port="1"/>
		<edge from-layer="176" from-port="2" to-layer="178" to-port="0"/>
		<edge from-layer="177" from-port="0" to-layer="178" to-port="1"/>
		<edge from-layer="178" from-port="2" to-layer="180" to-port="0"/>
		<edge from-layer="179" from-port="0" to-layer="180" to-port="1"/>
		<edge from-layer="180" from-port="2" to-layer="181" to-port="0"/>
		<edge from-layer="181" from-port="2" to-layer="182" to-port="0"/>
		<edge from-layer="182" from-port="1" to-layer="184" to-port="0"/>
		<edge from-layer="182" from-port="1" to-layer="189" to-port="1"/>
		<edge from-layer="183" from-port="0" to-layer="184" to-port="1"/>
		<edge from-layer="184" from-port="2" to-layer="186" to-port="0"/>
		<edge from-layer="185" from-port="0" to-layer="186" to-port="1"/>
		<edge from-layer="186" from-port="2" to-layer="188" to-port="0"/>
		<edge from-layer="187" from-port="0" to-layer="188" to-port="1"/>
		<edge from-layer="188" from-port="2" to-layer="189" to-port="0"/>
		<edge from-layer="189" from-port="2" to-layer="190" to-port="0"/>
		<edge from-layer="190" from-port="1" to-layer="197" to-port="1"/>
		<edge from-layer="190" from-port="1" to-layer="192" to-port="0"/>
		<edge from-layer="191" from-port="0" to-layer="192" to-port="1"/>
		<edge from-layer="192" from-port="2" to-layer="194" to-port="0"/>
		<edge from-layer="193" from-port="0" to-layer="194" to-port="1"/>
		<edge from-layer="194" from-port="2" to-layer="196" to-port="0"/>
		<edge from-layer="195" from-port="0" to-layer="196" to-port="1"/>
		<edge from-layer="196" from-port="2" to-layer="197" to-port="0"/>
		<edge from-layer="197" from-port="2" to-layer="198" to-port="0"/>
		<edge from-layer="198" from-port="1" to-layer="200" to-port="0"/>
		<edge from-layer="198" from-port="1" to-layer="226" to-port="0"/>
		<edge from-layer="199" from-port="0" to-layer="200" to-port="1"/>
		<edge from-layer="200" from-port="2" to-layer="202" to-port="0"/>
		<edge from-layer="201" from-port="0" to-layer="202" to-port="1"/>
		<edge from-layer="202" from-port="2" to-layer="204" to-port="0"/>
		<edge from-layer="203" from-port="0" to-layer="204" to-port="1"/>
		<edge from-layer="204" from-port="2" to-layer="210" to-port="0"/>
		<edge from-layer="205" from-port="0" to-layer="206" to-port="1"/>
		<edge from-layer="206" from-port="3" to-layer="209" to-port="0"/>
		<edge from-layer="207" from-port="0" to-layer="209" to-port="1"/>
		<edge from-layer="208" from-port="0" to-layer="209" to-port="2"/>
		<edge from-layer="209" from-port="3" to-layer="210" to-port="1"/>
		<edge from-layer="210" from-port="2" to-layer="211" to-port="1"/>
		<edge from-layer="211" from-port="2" to-layer="212" to-port="0"/>
		<edge from-layer="213" from-port="0" to-layer="214" to-port="1"/>
		<edge from-layer="214" from-port="2" to-layer="216" to-port="0"/>
		<edge from-layer="215" from-port="0" to-layer="216" to-port="1"/>
		<edge from-layer="216" from-port="2" to-layer="218" to-port="0"/>
		<edge from-layer="217" from-port="0" to-layer="218" to-port="1"/>
		<edge from-layer="218" from-port="2" to-layer="224" to-port="0"/>
		<edge from-layer="219" from-port="0" to-layer="220" to-port="1"/>
		<edge from-layer="220" from-port="3" to-layer="223" to-port="0"/>
		<edge from-layer="221" from-port="0" to-layer="223" to-port="1"/>
		<edge from-layer="222" from-port="0" to-layer="223" to-port="2"/>
		<edge from-layer="223" from-port="3" to-layer="224" to-port="1"/>
		<edge from-layer="224" from-port="2" to-layer="237" to-port="0"/>
		<edge from-layer="225" from-port="0" to-layer="226" to-port="1"/>
		<edge from-layer="226" from-port="2" to-layer="228" to-port="0"/>
		<edge from-layer="227" from-port="0" to-layer="228" to-port="1"/>
		<edge from-layer="228" from-port="2" to-layer="230" to-port="0"/>
		<edge from-layer="229" from-port="0" to-layer="230" to-port="1"/>
		<edge from-layer="230" from-port="2" to-layer="236" to-port="0"/>
		<edge from-layer="231" from-port="0" to-layer="232" to-port="1"/>
		<edge from-layer="232" from-port="3" to-layer="235" to-port="0"/>
		<edge from-layer="233" from-port="0" to-layer="235" to-port="1"/>
		<edge from-layer="234" from-port="0" to-layer="235" to-port="2"/>
		<edge from-layer="235" from-port="3" to-layer="236" to-port="1"/>
		<edge from-layer="236" from-port="2" to-layer="237" to-port="1"/>
		<edge from-layer="237" from-port="2" to-layer="238" to-port="0"/>
	</edges>
	<meta_data>
		<MO_version value="2022.1.0-7019-cdb9bec7210-releases/2022/1"/>
		<Runtime_version value="2022.1.0-7019-cdb9bec7210-releases/2022/1"/>
		<legacy_path value="False"/>
		<cli_parameters>
			<caffe_parser_path value="DIR"/>
			<compress_fp16 value="False"/>
			<data_type value="float"/>
			<disable_nhwc_to_nchw value="False"/>
			<disable_omitting_optional value="False"/>
			<disable_resnet_optimization value="False"/>
			<disable_weights_compression value="False"/>
			<enable_concat_optimization value="False"/>
			<enable_flattening_nested_params value="False"/>
			<enable_ssd_gluoncv value="False"/>
			<extensions value="DIR"/>
			<framework value="onnx"/>
			<freeze_placeholder_with_value value="{}"/>
			<input_model value="DIR/face_detection.onnx"/>
			<input_model_is_text value="False"/>
			<k value="DIR/CustomLayersMapping.xml"/>
			<layout value="()"/>
			<layout_values value="{}"/>
			<legacy_mxnet_model value="False"/>
			<log_level value="ERROR"/>
			<mean_scale_values value="{}"/>
			<mean_values value="()"/>
			<model_name value="face_detection"/>
			<output_dir value="DIR"/>
			<placeholder_data_types value="{}"/>
			<progress value="False"/>
			<remove_memory value="False"/>
			<remove_output_softmax value="False"/>
			<reverse_input_channels value="False"/>
			<save_params_from_nd value="False"/>
			<scale_values value="()"/>
			<silent value="False"/>
			<source_layout value="()"/>
			<static_shape value="False"/>
			<stream_output value="False"/>
			<target_layout value="()"/>
			<transform value=""/>
			<use_legacy_frontend value="False"/>
			<use_new_frontend value="False"/>
			<unset unset_cli_parameters="batch, counts, disable_fusing, finegrain_fusing, input, input_checkpoint, input_meta_graph, input_proto, input_shape, input_symbol, mean_file, mean_file_offsets, nd_prefix_name, output, placeholder_shapes, pretrained_model_name, saved_model_dir, saved_model_tags, scale, tensorboard_logdir, tensorflow_custom_layer_libraries, tensorflow_custom_operations_config_update, tensorflow_object_detection_api_pipeline_config, tensorflow_use_custom_operations_config, transformations_config"/>
		</cli_parameters>
	</meta_data>
</net>
