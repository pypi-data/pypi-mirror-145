model.init.name: |1+
    Human readable name
                or label of
                the PDF for better identification.
                Has no programmatical functional purpose as idendification.
model.init.obs: |1+
    Observables of the
                model. This will be used as the default space of the PDF and,
                if not given explicitly, as the normalization range.

                The default space is used for example in the sample method: if no
                sampling limits are given, the default space is used.

                The observables are not equal to the domain as it does not restrict or
                truncate the model outside of this range.

pdf.kde.bandwidth.weights: |1+
  Weights of each event
                in *data*, can be None or Tensor-like with shape compatible
                with *data*. This will change the count of the events, whereas
                weight :math:`w_i` of :math:`x_i`.


pdf.kde.bandwidth.data: |1+
  Data points to determine the bandwidth
                from.

pdf.kde.init.data: |1+
  Data sample to approximate
              the density from. The points represent positions of the *kernel*,
              the :math:`x_i`. This is preferrably a ``ZfitData``, but can also
              be an array-like object.

              If the data has weights, they will be taken into account.
              This will change the count of the events, whereas
              weight :math:`w_i` of :math:`x_i` will scale the value of
              :math:`K_i( x_i)`, resulting in a factor of :math:`\frac{w_i}{\sum w_i} `.

              If no weights are given, each kernel will be scaled by the same
              constant :math:`\frac{1}{n_{data}}`.


pdf.kde.init.obs: |1+
  Observable space of the KDE.
              As with any other PDF, this will be used as the default *norm*, but
              does not define the domain of the PDF. Namely this can be a smaller
              space than *data*, as long as the name of the observable match.
              Using a larger dataset is actually good practice to avoid
              bountary biases, see also :ref:`sec-boundary-bias-and-padding`.

pdf.kde.init.bandwidth: |1+
  Bandwidth of the kernel,
              often also denoted as :math:`h`. For a Gaussian kernel, this
              corresponds to *sigma*. This can be calculated using
              pre-defined options or by specifying a numerical value that is
              broadcastable to *data* -- a scalar or an array-like
              object with the same size as *data*.

              A scalar value is usually referred to as a global bandwidth while
              an array holds local bandwidths

pdf.kde.init.kernel: |1+
  The kernel is the heart
              of the Kernel Density Estimation, which consists of the sum of
              kernels around each sample point. Therefore, a kernel should represent
              the distribution probability of a single data point as close as
              possible.

              The most widespread kernel is a Gaussian, or Normal, distribution. Due
              to the law of large numbers, the sum of many (arbitrary) random variables
              -- this is the case for most real world observable as they are the result of
              multiple consecutive random effects -- results in a Gaussian distribution.
              However, there are many cases where this assumption is not per-se true. In
              this cases an alternative kernel may offers a better choice.

              Valid choices are callables that return a
              :py:class:`~tensorflow_probability.distribution.Distribution`, such as all distributions
              that belong to the loc-scale family.

pdf.kde.init.padding: |1+
  KDEs have a peculiar
              weakness: the boundaries, as the outside has a zero density. This makes the KDE
              go down at the bountary as well, as the density approaches zero, no matter what the
              density inside the boundary was.

              There are two ways to circumvent this problem:

                - the best solution: providing a larger dataset than the default space the PDF is used in
                - mirroring the existing data at the boundaries, which is equivalent to a boundary condition
                  with a zero derivative. This is a padding technique and can improve the boundaries.
                  However, one important drawback of this method is to keep in mind that this will actually
                  alter the PDF *to look mirrored*. If the PDF is plotted in a larger range, this becomes
                  clear.

              Possible options are a number (default 0.1) that depicts the fraction of the overall space
              that defines the data mirrored on both sides. For example, for a space from 0 to 5, a value of
              0.3 means that all data in the region of 0 to 1.5 is taken, mirrored around 0 as well as
              all data from 3.5 to 5 and mirrored at 5. The new data will go from -1.5 to 6.5, so the
              KDE is also having a shape outside the desired range. Using it only for the range 0 to 5
              hides this.
              Using a dict, each side separately (or only a single one) can be mirrored, like ``{'lowermirror: 0.1}``
              or ``{'lowermirror: 0.2, 'uppermirror': 0.1}``. For more control, a callable that takes data and weights can
              also be used.


pdf.kde.init.weights: |1+
  Weights of each event
              in *data*, can be None or Tensor-like with shape compatible
              with *data*. Instead of using this parameter, it is preferred
              to use a ``ZfitData`` as *data* that contains weights.
              This will change the count of the events, whereas
              weight :math:`w_i` of :math:`x_i` will scale the value of :math:`K_i( x_i)`,
              resulting in a factor of :math:`\frac{w_i}{\sum w_i} `.

              If no weights are given, each kernel will be scaled by the same
              constant :math:`\frac{1}{n_{data}}`.

pdf.kde.init.num_grid_points: |1+
  Number of points in
              the binning grid.

              The data will be binned using the *binning_method* in *num_grid_points*
              and this histogram grid will then be used as kernel points. This has the
              advantage to have a constant computational complexity independent of the data
              size.

              A number from 32 on can already yield good results, while the default is set
              to 1024, creating a fine grid. Lowering the number increases the performance
              at the cost of accuracy.

pdf.kde.init.binning_method: |1+
  Method to be used for
              binning the data. Options are 'linear', 'simple'.

              The data can be binned in the usual way ('simple'), but this is less precise
              for KDEs, where we are interested in the shape of the histogram and smoothing
              it. Therefore, a better suited method, 'linear', is available.

              In normal binnig, each event (or weight) falls into the bin within the bin edges,
              while the neighbouring bins get zero counts from this event.
              In linear binning, the event is split between two bins, proportional to its
              closeness to each bin.

              The 'linear' method provides superior performance, most notably in small (~32)
              grids.

pdf.kde.bandwidth.explain_global: |1+
  A global bandwidth
              is a single parameter that is shared amongst all kernels.
              While this is a fast and robust method,
              it is a rule of thumb approximation. Due to its global nature,
              it cannot take into account the different varying
              local densities. It uses notably the least amount of memory
              of all methods.

pdf.kde.bandwidth.explain_local: |1+
  A local bandwidth
              means that each kernel i has a different bandwidth.
              In other words, given some data points with size n,
              we will need n bandwidth parameters.
              This is often more accurate than a global bandwidth,
              as it allows to have larger bandwiths in areas of smaller density,
              where, due to the small local sample size, we have less certainty
              over the true density while having a smaller bandwidth in denser
              populated areas.

              The accuracy comes at the cost of a longer pre-calculation to obtain
              the local bandwidth (there are different methods available), an
              increased runtime and - most importantly - an peak memory usage.

              This can be especially costly for a large number (> few thousand) of
              kernels and/or evaluating on large datasets (> 10'000).

pdf.kde.bandwidth.explain_adaptive: |1+
  Adaptive bandwidths are
              a way to reduce the dependence on the bandwidth parameter
              and usually are local bandwidths that take into account
              the local densities.
              Adaptive bandwidths are constructed by using an initial estimate
              of the local density in order to calculate a sensible bandwidth
              for each kernel. The initial estimator can be a kernel density
              estimation using a global bandwidth with a rule of thumb.
              The adaptive bandwidth h is obtained using this estimate, where
              usually

              .. math::

                h_{i} \propto f( x_{i} ) ^ {-1/2}

              Estimates can still differ in the overall scaling of this
              bandwidth.

minimizer.verbosity: |1+
 Verbosity of the minimizer. Has to be between 0 and 10.
               The verbosity has the meaning:

                - a value of 0 means quiet and no output
                - above 0 up to 5, information that is good to know but without
                  flooding the user, corresponding to a "INFO" level.
                - A value above 5 starts printing out considerably more and
                  is used more for debugging purposes.
                - Setting the verbosity to 10 will print out every
                  evaluation of the loss function and gradient.

                Some minimizer offer additional output which is also
                distributed as above but may duplicate certain printed values.

minimizer.tol: |1+
    Termination value for the
                    convergence/stopping criterion of the algorithm
                    in order to determine if the minimum has
                    been found. Defaults to 1e-3.
minimizer.criterion: |1+
    Criterion of the minimum. This is an
                    estimated measure for the distance to the
                    minimum and can include the relative
                    or absolute changes of the parameters,
                    function value, gradients and more.
                    If the value of the criterion is smaller
                    than ``loss.errordef * tol``, the algorithm
                    stopps and it is assumed that the minimum
                    has been found.
minimizer.strategy: |1+
    A class of type `ZfitStrategy` that takes no
                    input arguments in the init. Determines the behavior of the minimizer in
                    certain situations, most notably when encountering
                    NaNs. It can also implement a callback function.
minimizer.maxiter: |1+
    Approximate number of iterations.
                    This corresponds to roughly the maximum number of
                    evaluations of the `value`, 'gradient` or `hessian`.
minimizer.name: |1+
    Human readable name of the minimizer.
minimizer.maxcor: |1+
    Maximum number of memory history to keep
                    when using a quasi-Newton update formula such as BFGS.
                    It is the number of gradients
                    to “remember” from previous optimization
                    steps: increasing it increases
                    the memory requirements but may speed up the convergence.
minimizer.init.maxls: |1+
    Maximum number of linesearch points.

minimizer.scipy.gradient: |1+
    Define the method to use for the gradient computation
                    that the minimizer should use. This can be the
                    gradient provided by the loss itself or
                    method from the minimizer.
                    In general, using the zfit provided automatic gradient is
                    more precise and needs less computation time for the
                    evaluation compared to a numerical method but it may not always be
                    possible. In this case, zfit switches to a generic, numerical gradient
                    which in general performs worse than if the minimizer has its own
                    numerical gradient.
                    The following are possible choices:

                    If set to ``False`` or ``'zfit'`` (or ``None``; default), the
                    gradient of the loss (usually the automatic gradient) will be used;
                    the minimizer won't use an internal algorithm.


minimizer.scipy.gradient.internal: |1+
    ``True`` tells the minimizer to use its default internal
                    gradient estimation. This can be specified more clearly using the
                    arguments ``'2-point'`` and ``'3-point'``, which specify the
                    numerical algorithm the minimizer should use in order to
                    estimate the gradient.
minimizer.scipy.hessian: |1+
    Define the method to use for the hessian computation
                    that the minimizer should use. This can be the
                    hessian provided by the loss itself or
                    method from the minimizer.

                    While the exact gradient can speed up the convergence and is
                    often beneficial, this ain't true for the computation of the
                    (inverse) Hessian matrix.
                    Due to the $n^2$ number of entries (compared to $n$ in the
                    gradient) from the $n$ parameters, this can grow quite
                    large and become computationally expensive.

                    Therefore, many algorithms use an approximated (inverse)
                    Hessian matrix making use of the gradient updates instead
                    of calculating the exact matrix. This turns out to be
                    precise enough and usually considerably speeds up the
                    convergence.

                    The following are possible choices:

                    If set to ``False`` or ``'zfit'``, the
                    hessian defined in the loss (usually using automatic differentiation)
                    will be used;
                    the minimizer won't use an internal algorithm.
minimizer.scipy.hessian.internal: |1+
    A :class:`~scipy.optimize.HessianUpdateStrategy` that holds
                    an approximation of the hessian. For example
                    :class:`~scipy.optimize.BFGS` (which performs usually best)
                    or :class:`~scipy.optimize.SR1`
                    (sometimes unstable updates).
                    ``True``  (or ``None``; default) tells the minimizer
                    to use its default internal
                    hessian approximation.
                    Arguments ``'2-point'`` and ``'3-point'`` specify which
                    numerical algorithm the minimizer should use in order to
                    estimate the hessian. This is only possible if the
                    gradient is provided by zfit and not an internal numerical
                    method is already used to determine it.

minimizer.scipy.info: |1+
    This implenemtation wraps the minimizers in
         `SciPy optimize <https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html>`_.
minimizer.trust.eta: |1+
    Trust region related acceptance
                    stringency for proposed steps.
minimizer.trust.init_trust_radius: |1+
    Initial trust-region radius.
minimizer.trust.max_trust_radius: |1+
    Maximum value of the trust-region radius.
                    No steps that are longer than this value will be proposed.

minimizer.nlopt.population: |1+
    The population size for the evolutionary algorithm.

minimizer.nlopt.info: |1+
    More information on the algorithm can be found
         `here <https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/>`_.

         This implenemtation uses internally the
         `NLopt library <https://nlopt.readthedocs.io/en/latest/>`_.
         It is a
         free/open-source library for nonlinear optimization,
         providing a common interface for a number of
         different free optimization routines available online as well as
         original implementations of various other algorithms.




loss.init.model: |1+
   PDFs that return the normalized probability for
                *data* under the given parameters.
                If multiple model and data are given, they will be used
                in the same order to do a simultaneous fit.

loss.init.data: |1+
   Dataset that will be given to the *model*.
                If multiple model and data are given, they will be used
                in the same order to do a simultaneous fit.

loss.init.constraints: |1+
   Auxiliary measurements that add a term to the loss
                or terms that restrict the loss in an other way such as penalties.
loss.init.options: |1+
   Additional options (as a dict) for the loss.
                Current possibilities include:

                - 'subtr_const' (default True): subtract from each points
                  log probability density a constant that
                  is approximately equal to the average log probability
                  density in the very first evaluation before
                  the summation. This brings the initial loss value closer to 0 and increases,
                  especially for large datasets, the numerical stability.

                  The value will be stored ith 'subtr_const_value' and can also be given
                  directly.

                  The subtraction should not affect the minimum as the absolute
                  value of the NLL is meaningless. However,
                  with this switch on, one cannot directly compare
                  different likelihoods ablolute value as the constant
                  may differs! Use `create_new` in order to have a comparable likelihood
                  between different losses


                These settings may extend over time. In order to make sure that a loss is the
                same under the same data, make sure to use `create_new` instead of instantiating
                a new loss as the former will automatically overtake any relevant constants
                and behavior.




result.init.loss: |1+
    The loss function that was minimized.
                Usually, but not necessary, contains
                also the pdf, data and constraints.
result.init.params: |1+
    Result of the fit where each
                :py:class:`~zfit.Parameter` key has the
                value from the minimum found by the minimizer.

result.init.minimizer: |1+
    Minimizer that was used to obtain this `FitResult` and will be used to
                    calculate certain errors. If the minimizer
                    is state-based (like "iminuit"), then this is a copy
                    and the state of other `FitResults` or of the *actual*
                    minimizer that performed the minimization
                    won't be altered.
result.init.valid: |1+
    Indicating whether the result is valid or not. This is the strongest
                    indication and serves as
                    the global flag. The reasons why a result may be
                    invalid can be arbitrary, including but not exclusive:

                    - parameter(s) at the limit
                    - maxiter reached without proper convergence
                    - the minimizer may even converged but it is known that this is only a local minimum

                    To indicate the reason for the invalidity, pass a message.
result.init.edm: |1+
    The estimated distance to minimum
                    which is the criterion value at the minimum.
result.init.fmin: |1+
    Value of the function at the minimum.
result.init.criterion: |1+
    Criterion that was used during the minimization.
                    This determines the estimated distance to the
                    minimum (edm)
result.init.status: |1+
    A status code (if available) that describes
                    the minimization termination. 0 means a valid
                    termination.
result.init.converged: |1+
    Whether the fit has successfully converged or not.
                    The result itself can still be an invalid minimum
                    such as if the parameters are at or close
                    to the limits or in case another minimum is found.
result.init.message: |1+
    Human readable message to indicate the reason
                    if the fitresult is not valid.
                    If the fit is valid, the message (should)
                    be an empty string (or None),
                    otherwise it should denote the reason for the invalidity.
result.init.info: |1+
    Additional information (if available)
                    such as *number of gradient function calls* or the
                    original minimizer return message.
                    This is a relatively free field and _no single field_
                    in it is guaranteed to be stable.
                    Some recommended fields:

                    - *original*: contains the original returned object
                      by the minimizer used internally.
                    - *optimizer*: the actual instance of the wrapped
                      optimizer (if available)
result.init.approx: |1+
    Collection of approximations found during
                    the minimization process such as gradient and hessian.
result.init.niter: |1+
    Approximate number of iterations ~= number
                    of function evaluations ~= number of gradient evaluations.
                    This is an approximated value and the exact meaning
                    can differ between different minimizers.
result.init.evaluator: |1+
    Loss evaluator that was used during the
                    minimization and that may contains information
                    about the last evaluations of the gradient
                    etc which can serve as approximations.

result.init.values: |1+
    Values of the parameters at the
                    found minimum.
